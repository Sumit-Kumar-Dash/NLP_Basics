{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "para =\"\"\"\"Today, on this beautiful day, thinking about Swami Vivekananda, I am happy to see young people.\n",
    "\n",
    "I want to share a thought with all the youth present here, i have met, so far, 11 million youth like you. I have seen their hopes, experienced their pain, walked with their aspirations and heard through their despair. From all this I learned something about youth.\n",
    "\n",
    "I learnt, every youth wants to be unique, that is, YOU! But the world all around you is doing its best day and night, to make you just everybody else. Now the question is whether you want to be YOU or everybody else.\n",
    "\n",
    "Being like everybody else is convenient at first glance, but not satisfying in long vision.\n",
    "\n",
    "The challenge therefore, is that you have to fight the hardest battle, which any human being can ever imagine to fight. And never stop fighting untill you arrive at your designated place, that is UNIQUE YOU.\n",
    "\n",
    "To get the Unique you, there is a big battle. There are four things you need to have, to win that battle. The first is to set the goal, the second is to acquire the knowledge continuously, third is work hard with devotion and fourth is perseverance. If you have these four tools then you will definitely become UNIQUE YOU.\n",
    "\n",
    "Being Unique will require excellence. Excellence is self-imposed, self- directory life long process.\n",
    "\n",
    "Excellence is not by accident. It is process, where an individual, organisation or nation continuously strive to better oneself. The performance standards are set by themselves, they work on their dreams with focus and are prepared to take calculated results and do not get deterred by failures as they move towards their dreams. Then, they step up their dreams as they tend to their potential, in the process they increase their potential, and this is an unending life cycle phenomenon. They are not in competition with anybody else, but themselves. That is the culture of Excellence.\n",
    "\n",
    "Students ought to think, and think well. They should do everything to build up a new state of India which would be everybodyâ€™s pride.\n",
    "\n",
    "I would like to ask yourself, what you would like to remembered for? You should write this on a page, and that page will be most important page in book of human history. And you will be remembered for creating that page in the history of nation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = nltk.sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemm=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "tweets_clean = []\n",
    "\n",
    "for i in range(len(sentence)): # Go through every word in your tokens list\n",
    "    word=nltk.word_tokenize(sentence[i])\n",
    "    for j in word:\n",
    "        if (j.lower() not in stopwords.words('english') and j.lower() not in string.punctuation):  # remove punctuation\n",
    "            words=[stemm.stem(j.lower())]\n",
    "            sentence[i]=\" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(para)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    for j in words:\n",
    "        if (j.lower() not in stopwords.words('english') and j.lower() not in string.punctuation):  # remove punctuation\n",
    "            wo=[lemmatizer.lemmatize(j.lower())]\n",
    "            sentences[i]=\" \".join(wo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(sentence).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the TF-IDF model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "Y = cv.fit_transform(sentence).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = nltk.sent_tokenize(para)\n",
    "\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',para)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "text = text.lower()\n",
    "text = re.sub(r'\\d',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "\n",
    "# Preparing the dataset\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]\n",
    "    \n",
    "    \n",
    "# Training the Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "\n",
    "#words = model.wv.vocab\n",
    "\n",
    "# Finding Word Vectors\n",
    "#vector = model.wv['war']\n",
    "\n",
    "# Most similar words\n",
    "similar = model.wv.most_similar('human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('satisfying', 0.2222307026386261),\n",
       " ('towards', 0.18042919039726257),\n",
       " ('potential', 0.17296582460403442),\n",
       " ('young', 0.16602866351604462),\n",
       " ('four', 0.1553310751914978),\n",
       " ('long', 0.14908424019813538),\n",
       " ('learnt', 0.145674467086792),\n",
       " ('untill', 0.14558309316635132),\n",
       " ('require', 0.13828395307064056),\n",
       " ('think', 0.13468290865421295)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
